{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 13: Neural Networks\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov, Jin Sun__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: An Artifical Neuron\n",
    "\n",
    "In this lecture, we will learn about a new class of machine learning algorithms inspired by the brain.\n",
    "\n",
    "We will start by defining a few building blocks for these algorithms, and draw connections to neuroscience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Components of A Supervised Machine Learning Problem\n",
    "\n",
    "At a high level, a supervised machine learning problem has the following structure:\n",
    "\n",
    "$$ \\underbrace{\\text{Training Dataset}}_\\text{Attributes + Features} + \\underbrace{\\text{Learning Algorithm}}_\\text{Model Class + Objective + Optimizer } \\to \\text{Predictive Model} $$\n",
    "\n",
    "Where does the dataset come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Binary Classification\n",
    "\n",
    "In supervised learning, we fit a model of the form\n",
    "$$ f : \\mathcal{X} \\to \\mathcal{Y} $$\n",
    "that maps inputs $x \\in \\mathcal{X}$ to targets $y \\in \\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In classification, the space of targets $\\mathcal{Y}$ is *discrete*. Classification is binary if $\\mathcal{Y} = \\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each value of $y$ value is a *class* and we are interested in finding a hyperplane that separates the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Logistic Regression\n",
    "\n",
    "Logistic regression fits a model of the form\n",
    "$$ f(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}, $$\n",
    "where\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)} $$\n",
    "is known as the *sigmoid* or *logistic* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is how the logistic function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1172c9160>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "z = np.linspace(-5, 5)\n",
    "sigma = 1/(1+np.exp(-z))\n",
    "\n",
    "plt.plot(z, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Biological Neuron\n",
    "\n",
    "In order to define an artifical neuron, let's look first a biological one.\n",
    "\n",
    "TODO: PUT NEURON IMAGE FROM HERE: https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "* Each neuron receives input signals from its dendrites\n",
    "* It produces output signals along its axon, which connects to the dendrites of other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Artificial Neuron: Example\n",
    "\n",
    "We can imitate this machinery using an idealized artifical neuron.\n",
    "* The neuron receives signals $x_j$ at dendrites, which are modulated multiplicatively: $w_j \\cdot x_j$.\n",
    "* The body of the neuron sums the modulated inputs: $\\sum_{j=1}^d w_j \\cdot x_j$.\n",
    "* These go into the activation function that produces an ouput.\n",
    "\n",
    "TODO: PUT ARTIFICIAL NEURON IMAGE FROM HERE: https://cs231n.github.io/neural-networks-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Artificial Neuron: Notation\n",
    "\n",
    "More formally, we say that a neuron is a model $f : \\mathbb{R}^d \\to [0,1]$, with the following components:\n",
    "* Inputs $x_1,x_2,...,x_d$, denoted by a vector $x$.\n",
    "* Weight vector $w \\in \\mathbb{R}^d$ that modulates input $x$ as $w^\\top x$.\n",
    "* An activation function $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ that computes the output $\\sigma(w^\\top x)$ of the neuron based on the sum of modulated features $w^\\top x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression as an Artifical Neuron\n",
    "\n",
    "Logistic regression is a model of the form\n",
    "$$ f(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}, $$\n",
    "that can be interpreted as a neuron that uses the *sigmoid* as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "Another model of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Need to implement a small example. Can probably copy-paste implementation of LR from the LR slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Let's list a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "# Part 2: Artificial Neural Networks\n",
    "\n",
    "Let's now see how we can connect neurons into networks that form complex models that further mimic the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Artificial Neuron\n",
    "\n",
    "We say that a neuron is a model $f : \\mathbb{R}^d \\to [0,1]$, with the following components:\n",
    "* Inputs $x_1,x_2,...,x_d$, denoted by a vector $x$.\n",
    "* Weight vector $w \\in \\mathbb{R}^d$ that modulates input $x$ as $w^\\top x$.\n",
    "* An activation function $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ that computes the output $\\sigma(w^\\top x)$ of the neuron based on the sum of modulated features $w^\\top x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Logistic Regression as Neuron\n",
    "\n",
    "Logistic regression is a model of the form\n",
    "$$ f(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}, $$\n",
    "that can be interpreted as a neuron that uses the *sigmoid* as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Intuition\n",
    "\n",
    "A neural network is a directed graph in which a node is a neuron that takes as input the outputs of the neurons that are connected to it.\n",
    "\n",
    "TODO: Add an image here. Maybe layer image from here: https://cs231n.github.io/neural-networks-1/ (It probably needs some annotations)\n",
    "\n",
    "Networks are typically organized in layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: Layers\n",
    "\n",
    "A neural network layer is a model $f : \\mathbb{R}^d \\to \\mathbb{R}^p$ that applies $p$ neurons ih parallel to an input $x$.\n",
    "$$ f(x) = \\begin{bmatrix}\n",
    "\\sigma(w_1^\\top x) \\\\\n",
    "\\sigma(w_2^\\top x) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(w_p^\\top x)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "where each $w_k$ is the vector of weights for the $k$-th neuron. We refer to $p$ as the *size* of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By combining the $w_k$ into one matrix $W$, we can write in a more succinct vectorized form:\n",
    "$$f(x) = \\sigma(W\\cdot x) = \\begin{bmatrix}\n",
    "\\sigma(w_1^\\top x) \\\\\n",
    "\\sigma(w_2^\\top x) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(w_p^\\top x)\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $\\sigma(W\\cdot x)_k = \\sigma(w_k^\\top x)$ and $W_{kj} = (w_k)_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Notation\n",
    "\n",
    "A neural network is a model $f : \\mathbb{R} \\to \\mathbb{R}$ that consists of a composition of $L$ neural network layers:\n",
    "$$ f(x) = f_L \\circ f_{L-1} \\circ \\ldots f_1 (x). $$\n",
    "The final layer $f_L$ has size one (assuming the neural net has one ouput); intermediary layers $f_l$ can have any number of neurons.\n",
    "\n",
    "The notation $f \\circ g(x)$ denotes the composition $f(g(x))$ of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a Neural Network\n",
    "\n",
    "* Let's implement a small neural net in the same that we implemented logistic regression\n",
    "* Then we just run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Neural Network Layers\n",
    "\n",
    "There are many types of neural network layers that can exist. Here are a few:\n",
    "* Ouput layer: normally has one neuron and special activation function that depends on the problem\n",
    "* Input layer: normally, this is just the input vector $x$.\n",
    "* Hidden layer: Any layer between input and output.\n",
    "* Dense layer: A layer in which every input is connected ot every neuron.\n",
    "* Convolutional layer: A layer in which the operation $w^\\top x$ implements a mathematical [convolution](https://en.wikipedia.org/wiki/Convolution).\n",
    "* Anything else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscience Angle\n",
    "\n",
    "Annything we should say here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "# Part 3: Backpropagation\n",
    "\n",
    "We have defined what is an artificial neural network.\n",
    "\n",
    "Let's not see how we can train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Neural Network Layers\n",
    "\n",
    "A neural network layer is a model $f : \\mathbb{R}^d \\to \\mathbb{R}^p$ that applies $p$ neurons ih parallel to an input $x$.\n",
    "$$f(x) = \\sigma(W\\cdot x) = \\begin{bmatrix}\n",
    "\\sigma(w_1^\\top x) \\\\\n",
    "\\sigma(w_2^\\top x) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(w_p^\\top x)\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where each $w_k$ is the vector of weights for the $k$-th neuron and $W_{kj} = (w_k)_j$. We refer to $p$ as the *size* of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Neural Networks\n",
    "\n",
    "A neural network is a model $f : \\mathbb{R} \\to \\mathbb{R}$ that consists of a composition of $L$ neural network layers:\n",
    "$$ f(x) = f_L \\circ f_{L-1} \\circ \\ldots f_1 (x). $$\n",
    "The final layer $f_L$ has size one (assuming the neural net has one ouput); intermediary layers $f_l$ can have any number of neurons.\n",
    "\n",
    "The notation $f \\circ g(x)$ denotes the composition $f(g(x))$ of functions\n",
    "\n",
    "TODO: Add some kind of image from the previous part of the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: The Gradient\n",
    "\n",
    "The gradient $\\nabla_\\theta f$ further extends the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at a point $\\theta_0$ as\n",
    "\n",
    "$$ \\nabla_\\theta f (\\theta_0) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "In other words, the $j$-th entry of the vector $\\nabla_\\theta f (\\theta_0)$ is the partial derivative $\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Gradient Descent\n",
    "\n",
    "If we want to optimize an objective $J(\\theta)$, we start with an initial guess $\\theta_0$ for the parameters and repeat the following update until the function is no longer decreasing:\n",
    "$$ \\theta_i := \\theta_{i-1} - \\alpha \\cdot \\nabla_\\theta J(\\theta_{i-1}). $$\n",
    "\n",
    "As code, this method may look as follows:\n",
    "```python\n",
    "theta, theta_prev = random_initialization()\n",
    "while abs(J(theta) - J(theta_prev)) > conv_threshold:\n",
    "    theta_prev = theta\n",
    "    theta = theta_prev - step_size * gradient(theta_prev)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "How do we apply gradient descent to a neural network?\n",
    "\n",
    "Explain backpropgation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Chain Rule of Calculus\n",
    "\n",
    "Probably will need to review this at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Let's implement backprop with the simple NN model we had earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "# Part 4: Stochastic Gradient Descent\n",
    "\n",
    "In practice, neural networks are often trained on very large datasets.\n",
    "\n",
    "This requires a mosification to the gradient descent algorithm that we have seen earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volodymyr will create this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-ode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "rise": {
   "controlsTutorial": false,
   "height": 900,
   "help": false,
   "margin": 0,
   "maxScale": 2,
   "minScale": 0.2,
   "progress": true,
   "scroll": true,
   "theme": "simple",
   "width": 1200
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
